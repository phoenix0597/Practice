# Дан несложный пример HTML-страницы: Sample Web Page.
#
# Изучите код этой страницы: https://www.columbia.edu/~fdc/sample.html и реализуйте программу,
# которая получает список всех подзаголовков сайта (они заключены в теги <h3>).
#
# Ожидаемый результат:
#
# ['CONTENTS', '1. Creating a Web Page', '2. HTML Syntax', '3. Special Characters', '4. Converting Plain Text to HTML',
# '5. Effects', '6. Lists', '7. Links', '8. Tables', '9. Viewing Your Web Page',
# '10. Installing Your Web Page on the Internet', '11. Where to go from here', '12. Postscript: Cell Phones']
#
#
#
# Сделайте так, чтобы программа работала для любого сайта, где есть такие теги.
import requests
from bs4 import BeautifulSoup


def get_h3_tags(url):
    """Получить все заголовки с тегами h3 с заданной веб-страницы.
    Args:
        url (str): URL веб-страницы.
    Returns:
        list: список тегов h3.
    """
    
    # Получить содержимое веб-страницы
    response = requests.get(url)
    
    # Создать объект BeautifulSoup для разбора HTML
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Найти все теги h3 на веб-странице
    h3_tags = soup.find_all("h3")
    
    # Извлечь текст из каждого тега h3
    h3_texts = [h3_tag.text for h3_tag in h3_tags]
    
    return h3_texts


if __name__ == '__main__':
    # URL веб-страницы
    web_page_url = 'https://www.columbia.edu/~fdc/sample.html'
    
    # Получить список тегов h3 с указанной веб-страницы
    h3_headers_list = get_h3_tags(web_page_url)
    
    # Вывести список тегов h3
    print(h3_headers_list)


# print('\n'.join(h3_headers_list))

# ### Цель программы:
# Эта программа написана для получения всех текстов, содержащихся в тегах `<h3>` с конкретной веб-страницы,
# указанной пользователем. Для достижения этой цели мы используем две библиотеки – `requests`
# для получения содержимого веб-страницы и `BeautifulSoup` из пакета `bs4` для разбора HTML-кода
# и извлечения нужной информации.
#
# ### Как это работает:
#
# 1. **Импорт необходимых библиотек:**
#    В начале кода мы импортируем библиотеку `requests` для отправки HTTP-запросов к веб-сайту,
#    а также `BeautifulSoup` из пакета `bs4` для удобной работы с HTML-документом, полученным в ответ.
#
# 2. **Определение функции `get_h3_tags`:**
#    Функция `get_h3_tags` принимает один аргумент – URL веб-страницы.
#    В этой функции реализуется основная логика программы:
#    - Сначала с помощью `requests.get(url)` выполняется HTTP-GET запрос к указанному URL.
#    - Затем содержимое веб-страницы (`response.text`) парсится с использованием объекта `BeautifulSoup`,
#    чтобы можно было удобно работать с структурой HTML-документа.
#    - Метод `find_all("h3")` применяется к объекту `soup` для поиска всех тегов `<h3>` в документе.
#    - С помощью генератора списка (`[h3_tag.text for h3_tag in h3_tags]`) из этих тегов извлекаются текстовые значения.
#
# 3. **Выполнение программы:**
#    - С помощью функции `input` программа запрашивает у пользователя URL и сохраняет его в переменную `url`.
#    - Далее вызывается функция `get_h3_tags` с указанным URL, в результате чего возвращается
#    список текстов всех найденных тегов `<h3>`.
#    - Наконец, программа печатает полученный список тегов `<h3>` на экран.

# ### Парсинг
#
#  – это процесс анализа и преобразования данных из одного формата в другой,
# более удобный формат для работы и обработки. В контексте веб-страниц и HTML,
# парсинг подразумевает анализ HTML-кода страницы и его структурирование в удобную для программной обработки форму.
#
# Когда мы говорим, что "содержимое веб-страницы (`response.text`) парсится", это означает,
# что текстовые данные HTML-документа, полученные в результате HTTP-запроса, обрабатываются таким образом,
# чтобы из них можно было легко извлекать конкретные элементы, работать с их атрибутами, текстом и так далее.
# Весь HTML-документ превращается из простого текста в структурированное дерево элементов,
# с которым гораздо удобнее работать при помощи программного кода.
#
# ### Как это работает в контексте BeautifulSoup
#
# BeautifulSoup – это библиотека на языке Python, предназначенная именно для парсинга HTML и XML документов.
# Она создает структуру, очень схожую по работе с DOM (Document Object Model) веб-страниц,
# что позволяет программистам легко выбирать определенные части страницы для дальнейшей работы.
#
# Когда мы передаем `response.text` в конструктор `BeautifulSoup`, библиотека анализирует переданный HTML-код
# и создает объект, в котором каждый тег, каждый атрибут и каждое текстовое содержимое становятся
# частями общей структурированной модели. Эта модель дает возможность использовать различные методы и свойства
# для извлечения информации: поиска элементов по тегам, классам, идентификаторам,
# извлечения текстового содержания элементов и так далее.
#
# К примеру, когда мы вызываем метод `find_all("h3")`, BeautifulSoup ищет во внутреннем структурированном представлении
# HTML-документа все элементы с тегом `h3` и возвращает их в виде списка объектов.
# Эти объекты уже удобно использовать в коде для дальнейшей обработки.
#
# Таким образом, процесс парсинга, о котором мы говорили, – это именно превращение обычного текста HTML-кода
# в структурируемые и удобные для манипуляций объекты в памяти компьютера,
# что и делает работу с веб-страницами при помощи программного кода гораздо проще и эффективнее.
